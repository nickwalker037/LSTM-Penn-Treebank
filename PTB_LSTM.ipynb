{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get Data: </h3>\n",
    "Download the Penn Treebank dataset from IBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "Archive:  data/ptb.zip\n",
      "  inflating: data/ptb/reader.py      \n",
      "  inflating: data/__MACOSX/ptb/._reader.py  \n",
      "  inflating: data/__MACOSX/._ptb     \n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "!unzip -o data/ptb.zip -d data\n",
    "!cp data/ptb/reader.py .\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download simple examples dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-08 18:12:29--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz.8’\n",
      "\n",
      "simple-examples.tgz 100%[=====================>]  33.25M  3.79MB/s   in 10s    \n",
      "\n",
      "2019-03-08 18:12:40 (3.30 MB/s) - ‘simple-examples.tgz.8’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Defining Hyperparameters: </h3>\n",
    "Here, we define the model's hypterparameters so that we can practice playing around with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_scale = 0.1                  # initial weight scale\n",
    "learning_rate = 1.0               # initial learning weight\n",
    "max_grad_norm = 5                 # max permissible norm for the gradient -- for Gradient Clipping\n",
    "num_layers = 2                    # number of layers in our model\n",
    "num_steps = 20                    # total number of recurrence steps \n",
    "\n",
    "hidden_size_l1 = 256              # number of neurons (processing units) in the hidden layers\n",
    "hidden_size_l2 = 128\n",
    "\n",
    "max_epoch_decay_lr = 4            # max number of epochs trained with the initial learning weight\n",
    "max_epoch = 15                    # total epochs in training\n",
    "\n",
    "keep_prob = 1                     # probability of keeping data in the Dropout layer\n",
    "decay = 0.5                       # the decay for the learning rate\n",
    "batch_size = 60                   # size for each batch of data\n",
    "vocab_size = 10000                # vocab size\n",
    "embedding_vector_size = 200       \n",
    "\n",
    "is_training = 1                   # training flag to separate training from testing\n",
    "data_dir = \"data/simple-examples/data/\" # data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> LSTM Model </h3>\n",
    "All the code in the other document shows how the model is built step by step. Now we can create a class combining all the steps in that document that represents our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    \n",
    "    def __init__(self, action_type):\n",
    "        \n",
    "        # setting parameters for ease of use\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_vector_size = embedding_vector_size\n",
    "        # ------------------------------------------------\n",
    "        \n",
    "        # creating placeholders for our input data and expected output\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[60x20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[60x20]\n",
    "        # -----------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        #############################\n",
    "        # Creating the LSTM structure\n",
    "        #############################\n",
    "            # and connecting this with the RNN structure\n",
    "            # here, no bias is added to the Forget gate\n",
    "            # the LSTM cell processes one word at a time and computes probabilities of the possible.. \n",
    "                # continuations of the sentence\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        \n",
    "        # creating the Dropout wrapper for our LSTM unit\n",
    "            # unless we changed keep_prob, this won't actually execute\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Creating the RNN structure. The RNN is composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "        # ----------------------------------------------------------------------\n",
    "        \n",
    "        # Defining the initial state.\n",
    "            # the memory of the network is initialized with a vector of zeros and gets updated after reading each word\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "        # -------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        ##################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data\n",
    "        ##################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embedding_vector_size])  # [10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "        \n",
    "        # Creating the Dropout Addition for our inputs\n",
    "            # this will only execute if we change keep_prob\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "        \n",
    "        \n",
    "        ##########################################\n",
    "        # Creating the input structure for our RNN\n",
    "        ##########################################\n",
    "        \n",
    "        \n",
    "        ##################################################################################################\n",
    "        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state \n",
    "        ##################################################################################################\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        #######################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word\n",
    "        #######################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        \n",
    "        ###############################################################\n",
    "        # Defining the loss and cost functions for our model's learning\n",
    "        ###############################################################\n",
    "        \n",
    "        # use the contrib seq loss and aerage over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "        \n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        \n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "        \n",
    "        # everything after this point is only relevant for training\n",
    "        if action_type != \"is_training\":\n",
    "            return\n",
    "        \n",
    "        #############################################\n",
    "        # Create the training operation for our model\n",
    "        #############################################\n",
    "        \n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    #########################################\n",
    "    # Helper functions for our LSTM RNN class\n",
    "    #########################################\n",
    "    \n",
    "    # assign learning rate for this model:\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "    \n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "    \n",
    "    # Returns the Initial State for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "    \n",
    "    # Returns the defined cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "    \n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "    \n",
    "    # Returns the training operation defined for this model:\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n",
    "\n",
    "What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our function takes the parameters: the current session, the model instance, the data to be fed, the operation to be run\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "    \n",
    "    # define epoch size based on the length of the data, batch size and number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    \n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    # For each step and data point:\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training, validation, and testing\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Running the Model: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Learning Rate: 1.000\n",
      "Itr 10 of 774, perplexity: 5085.076 speed: 1355 wps\n",
      "Itr 87 of 774, perplexity: 1305.121 speed: 1363 wps\n",
      "Itr 164 of 774, perplexity: 998.157 speed: 1359 wps\n",
      "Itr 241 of 774, perplexity: 825.813 speed: 1356 wps\n",
      "Itr 318 of 774, perplexity: 727.719 speed: 1357 wps\n",
      "Itr 395 of 774, perplexity: 650.444 speed: 1357 wps\n",
      "Itr 472 of 774, perplexity: 587.534 speed: 1355 wps\n",
      "Itr 549 of 774, perplexity: 532.543 speed: 1355 wps\n",
      "Itr 626 of 774, perplexity: 488.792 speed: 1356 wps\n",
      "Itr 703 of 774, perplexity: 454.458 speed: 1357 wps\n",
      "Epoch 1 : Train Perplexity: 430.138\n",
      "Epoch 1 : Valid Perplexity: 235.147\n",
      "Epoch 2: Learning Rate: 1.000\n",
      "Itr 10 of 774, perplexity: 271.578 speed: 1333 wps\n",
      "Itr 87 of 774, perplexity: 237.939 speed: 1345 wps\n",
      "Itr 164 of 774, perplexity: 228.008 speed: 1347 wps\n",
      "Itr 241 of 774, perplexity: 218.474 speed: 1350 wps\n",
      "Itr 318 of 774, perplexity: 215.992 speed: 1352 wps\n",
      "Itr 395 of 774, perplexity: 210.086 speed: 1352 wps\n",
      "Itr 472 of 774, perplexity: 205.738 speed: 1354 wps\n",
      "Itr 549 of 774, perplexity: 199.044 speed: 1354 wps\n",
      "Itr 626 of 774, perplexity: 193.526 speed: 1355 wps\n",
      "Itr 703 of 774, perplexity: 189.313 speed: 1356 wps\n",
      "Epoch 2 : Train Perplexity: 186.501\n",
      "Epoch 2 : Valid Perplexity: 172.146\n",
      "Epoch 3: Learning Rate: 1.000\n",
      "Itr 10 of 774, perplexity: 184.481 speed: 1321 wps\n",
      "Itr 87 of 774, perplexity: 159.885 speed: 1359 wps\n",
      "Itr 164 of 774, perplexity: 155.806 speed: 1362 wps\n",
      "Itr 241 of 774, perplexity: 151.094 speed: 1369 wps\n",
      "Itr 318 of 774, perplexity: 151.203 speed: 1364 wps\n",
      "Itr 395 of 774, perplexity: 148.477 speed: 1363 wps\n",
      "Itr 472 of 774, perplexity: 146.951 speed: 1363 wps\n",
      "Itr 549 of 774, perplexity: 143.307 speed: 1363 wps\n",
      "Itr 626 of 774, perplexity: 140.446 speed: 1363 wps\n",
      "Itr 703 of 774, perplexity: 138.576 speed: 1363 wps\n",
      "Epoch 3 : Train Perplexity: 137.490\n",
      "Epoch 3 : Valid Perplexity: 153.101\n",
      "Epoch 4: Learning Rate: 1.000\n",
      "Itr 10 of 774, perplexity: 148.285 speed: 1329 wps\n",
      "Itr 87 of 774, perplexity: 127.024 speed: 1356 wps\n",
      "Itr 164 of 774, perplexity: 125.086 speed: 1356 wps\n",
      "Itr 241 of 774, perplexity: 121.775 speed: 1355 wps\n",
      "Itr 318 of 774, perplexity: 122.627 speed: 1355 wps\n",
      "Itr 395 of 774, perplexity: 120.630 speed: 1355 wps\n",
      "Itr 472 of 774, perplexity: 119.812 speed: 1358 wps\n",
      "Itr 549 of 774, perplexity: 117.146 speed: 1360 wps\n",
      "Itr 626 of 774, perplexity: 115.191 speed: 1361 wps\n",
      "Itr 703 of 774, perplexity: 114.081 speed: 1361 wps\n",
      "Epoch 4 : Train Perplexity: 113.546\n",
      "Epoch 4 : Valid Perplexity: 144.218\n",
      "Epoch 5: Learning Rate: 1.000\n",
      "Itr 10 of 774, perplexity: 126.867 speed: 1363 wps\n",
      "Itr 87 of 774, perplexity: 108.469 speed: 1358 wps\n",
      "Itr 164 of 774, perplexity: 107.167 speed: 1360 wps\n",
      "Itr 241 of 774, perplexity: 104.672 speed: 1360 wps\n",
      "Itr 318 of 774, perplexity: 105.713 speed: 1361 wps\n",
      "Itr 395 of 774, perplexity: 104.181 speed: 1361 wps\n",
      "Itr 472 of 774, perplexity: 103.673 speed: 1362 wps\n",
      "Itr 549 of 774, perplexity: 101.449 speed: 1363 wps\n",
      "Itr 626 of 774, perplexity: 99.982 speed: 1363 wps\n",
      "Itr 703 of 774, perplexity: 99.206 speed: 1362 wps\n",
      "Epoch 5 : Train Perplexity: 98.905\n",
      "Epoch 5 : Valid Perplexity: 138.606\n",
      "Epoch 6: Learning Rate: 0.500\n",
      "Itr 10 of 774, perplexity: 109.832 speed: 1327 wps\n",
      "Itr 87 of 774, perplexity: 93.063 speed: 1358 wps\n",
      "Itr 164 of 774, perplexity: 91.164 speed: 1359 wps\n",
      "Itr 241 of 774, perplexity: 88.380 speed: 1363 wps\n",
      "Itr 318 of 774, perplexity: 88.724 speed: 1360 wps\n",
      "Itr 395 of 774, perplexity: 86.910 speed: 1358 wps\n",
      "Itr 472 of 774, perplexity: 86.146 speed: 1357 wps\n",
      "Itr 549 of 774, perplexity: 83.826 speed: 1358 wps\n",
      "Itr 626 of 774, perplexity: 82.130 speed: 1359 wps\n",
      "Itr 703 of 774, perplexity: 81.102 speed: 1361 wps\n",
      "Epoch 6 : Train Perplexity: 80.504\n",
      "Epoch 6 : Valid Perplexity: 127.424\n",
      "Epoch 7: Learning Rate: 0.250\n",
      "Itr 10 of 774, perplexity: 95.367 speed: 1358 wps\n",
      "Itr 87 of 774, perplexity: 82.143 speed: 1367 wps\n",
      "Itr 164 of 774, perplexity: 80.644 speed: 1365 wps\n",
      "Itr 241 of 774, perplexity: 78.141 speed: 1363 wps\n",
      "Itr 318 of 774, perplexity: 78.460 speed: 1362 wps\n",
      "Itr 395 of 774, perplexity: 76.759 speed: 1362 wps\n",
      "Itr 472 of 774, perplexity: 75.999 speed: 1363 wps\n",
      "Itr 549 of 774, perplexity: 73.860 speed: 1363 wps\n",
      "Itr 626 of 774, perplexity: 72.241 speed: 1363 wps\n",
      "Itr 703 of 774, perplexity: 71.201 speed: 1363 wps\n",
      "Epoch 7 : Train Perplexity: 70.528\n",
      "Epoch 7 : Valid Perplexity: 124.785\n",
      "Epoch 8: Learning Rate: 0.125\n",
      "Itr 10 of 774, perplexity: 87.591 speed: 1350 wps\n",
      "Itr 87 of 774, perplexity: 75.985 speed: 1353 wps\n",
      "Itr 164 of 774, perplexity: 74.701 speed: 1357 wps\n",
      "Itr 241 of 774, perplexity: 72.417 speed: 1362 wps\n",
      "Itr 318 of 774, perplexity: 72.761 speed: 1361 wps\n",
      "Itr 395 of 774, perplexity: 71.174 speed: 1359 wps\n",
      "Itr 472 of 774, perplexity: 70.442 speed: 1358 wps\n",
      "Itr 549 of 774, perplexity: 68.410 speed: 1360 wps\n",
      "Itr 626 of 774, perplexity: 66.857 speed: 1359 wps\n",
      "Itr 703 of 774, perplexity: 65.833 speed: 1358 wps\n",
      "Epoch 8 : Train Perplexity: 65.148\n",
      "Epoch 8 : Valid Perplexity: 123.858\n",
      "Epoch 9: Learning Rate: 0.062\n",
      "Itr 10 of 774, perplexity: 83.274 speed: 1364 wps\n",
      "Itr 87 of 774, perplexity: 72.525 speed: 1353 wps\n",
      "Itr 164 of 774, perplexity: 71.370 speed: 1356 wps\n",
      "Itr 241 of 774, perplexity: 69.232 speed: 1352 wps\n",
      "Itr 318 of 774, perplexity: 69.602 speed: 1353 wps\n",
      "Itr 395 of 774, perplexity: 68.076 speed: 1356 wps\n",
      "Itr 472 of 774, perplexity: 67.376 speed: 1347 wps\n",
      "Itr 549 of 774, perplexity: 65.415 speed: 1345 wps\n",
      "Itr 626 of 774, perplexity: 63.907 speed: 1347 wps\n",
      "Itr 703 of 774, perplexity: 62.903 speed: 1348 wps\n",
      "Epoch 9 : Train Perplexity: 62.230\n",
      "Epoch 9 : Valid Perplexity: 123.205\n",
      "Epoch 10: Learning Rate: 0.031\n",
      "Itr 10 of 774, perplexity: 81.091 speed: 1335 wps\n",
      "Itr 87 of 774, perplexity: 70.666 speed: 1356 wps\n",
      "Itr 164 of 774, perplexity: 69.541 speed: 1357 wps\n",
      "Itr 241 of 774, perplexity: 67.493 speed: 1358 wps\n",
      "Itr 318 of 774, perplexity: 67.881 speed: 1357 wps\n",
      "Itr 395 of 774, perplexity: 66.391 speed: 1355 wps\n",
      "Itr 472 of 774, perplexity: 65.706 speed: 1353 wps\n",
      "Itr 549 of 774, perplexity: 63.783 speed: 1352 wps\n",
      "Itr 626 of 774, perplexity: 62.300 speed: 1352 wps\n",
      "Itr 703 of 774, perplexity: 61.307 speed: 1353 wps\n",
      "Epoch 10 : Train Perplexity: 60.644\n",
      "Epoch 10 : Valid Perplexity: 122.768\n",
      "Epoch 11: Learning Rate: 0.016\n",
      "Itr 10 of 774, perplexity: 79.904 speed: 1342 wps\n",
      "Itr 87 of 774, perplexity: 69.647 speed: 1362 wps\n",
      "Itr 164 of 774, perplexity: 68.532 speed: 1344 wps\n",
      "Itr 241 of 774, perplexity: 66.535 speed: 1337 wps\n",
      "Itr 318 of 774, perplexity: 66.930 speed: 1339 wps\n",
      "Itr 395 of 774, perplexity: 65.469 speed: 1343 wps\n",
      "Itr 472 of 774, perplexity: 64.793 speed: 1346 wps\n",
      "Itr 549 of 774, perplexity: 62.891 speed: 1349 wps\n",
      "Itr 626 of 774, perplexity: 61.422 speed: 1349 wps\n",
      "Itr 703 of 774, perplexity: 60.435 speed: 1350 wps\n",
      "Epoch 11 : Train Perplexity: 59.778\n",
      "Epoch 11 : Valid Perplexity: 122.489\n",
      "Epoch 12: Learning Rate: 0.008\n",
      "Itr 10 of 774, perplexity: 79.232 speed: 1357 wps\n",
      "Itr 87 of 774, perplexity: 69.073 speed: 1347 wps\n",
      "Itr 164 of 774, perplexity: 67.977 speed: 1342 wps\n",
      "Itr 241 of 774, perplexity: 66.005 speed: 1347 wps\n",
      "Itr 318 of 774, perplexity: 66.403 speed: 1349 wps\n",
      "Itr 395 of 774, perplexity: 64.962 speed: 1350 wps\n",
      "Itr 472 of 774, perplexity: 64.292 speed: 1352 wps\n",
      "Itr 549 of 774, perplexity: 62.403 speed: 1354 wps\n",
      "Itr 626 of 774, perplexity: 60.942 speed: 1353 wps\n",
      "Itr 703 of 774, perplexity: 59.958 speed: 1352 wps\n",
      "Epoch 12 : Train Perplexity: 59.304\n",
      "Epoch 12 : Valid Perplexity: 122.285\n",
      "Epoch 13: Learning Rate: 0.004\n",
      "Itr 10 of 774, perplexity: 78.837 speed: 1344 wps\n",
      "Itr 87 of 774, perplexity: 68.750 speed: 1338 wps\n",
      "Itr 164 of 774, perplexity: 67.667 speed: 1342 wps\n",
      "Itr 241 of 774, perplexity: 65.714 speed: 1345 wps\n",
      "Itr 318 of 774, perplexity: 66.114 speed: 1344 wps\n",
      "Itr 395 of 774, perplexity: 64.683 speed: 1342 wps\n",
      "Itr 472 of 774, perplexity: 64.016 speed: 1342 wps\n",
      "Itr 549 of 774, perplexity: 62.137 speed: 1341 wps\n",
      "Itr 626 of 774, perplexity: 60.680 speed: 1341 wps\n",
      "Itr 703 of 774, perplexity: 59.698 speed: 1340 wps\n",
      "Epoch 13 : Train Perplexity: 59.046\n",
      "Epoch 13 : Valid Perplexity: 122.124\n",
      "Epoch 14: Learning Rate: 0.002\n",
      "Itr 10 of 774, perplexity: 78.609 speed: 1308 wps\n",
      "Itr 87 of 774, perplexity: 68.571 speed: 1348 wps\n",
      "Itr 164 of 774, perplexity: 67.493 speed: 1339 wps\n",
      "Itr 241 of 774, perplexity: 65.553 speed: 1342 wps\n",
      "Itr 318 of 774, perplexity: 65.956 speed: 1344 wps\n",
      "Itr 395 of 774, perplexity: 64.530 speed: 1345 wps\n",
      "Itr 472 of 774, perplexity: 63.866 speed: 1344 wps\n",
      "Itr 549 of 774, perplexity: 61.992 speed: 1345 wps\n",
      "Itr 626 of 774, perplexity: 60.538 speed: 1347 wps\n",
      "Itr 703 of 774, perplexity: 59.557 speed: 1347 wps\n",
      "Epoch 14 : Train Perplexity: 58.907\n",
      "Epoch 14 : Valid Perplexity: 122.003\n",
      "Epoch 15: Learning Rate: 0.001\n",
      "Itr 10 of 774, perplexity: 78.484 speed: 1349 wps\n",
      "Itr 87 of 774, perplexity: 68.471 speed: 1350 wps\n",
      "Itr 164 of 774, perplexity: 67.395 speed: 1349 wps\n",
      "Itr 241 of 774, perplexity: 65.462 speed: 1350 wps\n",
      "Itr 318 of 774, perplexity: 65.868 speed: 1352 wps\n",
      "Itr 395 of 774, perplexity: 64.447 speed: 1355 wps\n",
      "Itr 472 of 774, perplexity: 63.785 speed: 1357 wps\n",
      "Itr 549 of 774, perplexity: 61.914 speed: 1356 wps\n",
      "Itr 626 of 774, perplexity: 60.462 speed: 1356 wps\n",
      "Itr 703 of 774, perplexity: 59.482 speed: 1356 wps\n",
      "Epoch 15 : Train Perplexity: 58.833\n",
      "Epoch 15 : Valid Perplexity: 121.931\n",
      "Test Perplexity: 117.184\n"
     ]
    }
   ],
   "source": [
    "# Initializes the execution graph and the Session:\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope adds a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    \n",
    "    # Reuses the trained parameters for the validation and testing models;\n",
    "    with tf.variable_scope(\"model\", reuse = True, initializer = initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "        \n",
    "    # initialize all variables:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "        \n",
    "        print(\"Epoch %d: Learning Rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "        \n",
    "    # Run the loop in the testing model to see how effective our training was:\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PTBModel' object has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0e2288a97b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/simple-examples/data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/simple-examples/data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PTBModel' object has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "m.save_weights(\"data/simple-examples/data/\")\n",
    "mtest.save_weights(\"data/simple-examples/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PTBModel' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-83995c462821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/simple-examples/data/train_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/simple-examples/data/train_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PTBModel' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "m.save(\"data/simple-examples/data/train_model.h5\")\n",
    "mtest.save(\"data/simple-examples/data/train_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
